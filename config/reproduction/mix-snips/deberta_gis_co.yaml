base:
  name: "OpenSLUv2"
  multi_intent: true
  train: true
  test: false
  device: cuda
  seed: 42
  epoch_num: 1000
  batch_size: 8
  ignore_index: -100

model_manager:
  load_dir: null
  save_dir: save/deberta-gis-co-mix-snips
  save_mode: save-by-eval

evaluator:
  best_key: EMA
  eval_by_epoch: true
  eval_step: 1
  metric:
    - intent_acc
    - intent_f1
    - slot_f1
    - EMA

accelerator:
  use_accelerator: true

dataset:
  dataset_name: mix-snips

tokenizer:
  _tokenizer_name_: microsoft/deberta-v3-base
  _padding_side_: right
  add_special_tokens: false
  max_length: 200

optimizer:
  _model_target_: torch.optim.AdamW
  _model_partial_: true
  lr: 5e-5
  weight_decay: 1e-8

scheduler:
  _model_target_: transformers.get_scheduler
  _model_partial_: true
  name : "linear"
  num_warmup_steps: 0

model:
  _model_target_: model.OpenSLUModel

  encoder:
    _model_target_: model.encoder.AutoEncoder
    encoder_name: microsoft/deberta-v3-base
    output_dim: 768
    return_with_input: true
    return_sentence_level_hidden: true

  decoder:
    _model_target_: model.decoder.BaseDecoder
    interaction:
      _model_target_: model.decoder.interaction.gis_co_interaction.GISCoInteraction
      level_projection_size: 32
      input_dim: "{model.encoder.output_dim}"
      hidden_dim: 256
      output_dim: "{model.decoder.interaction.hidden_dim}"
      slot_label_num: "{base.intent_label_num}"
      intent_label_num: "{base.intent_label_num}"
      dropout_rate: 0.4
      intent_embedding_dim: 64
      self_attention_hidden_dim: 256
      slot_embedding_dim: 128
      num_attention_heads: 8
      co_occurrence:
        data_dir: common/co_occurence/MixAtis
        save_dir: save/co_occurence/MixAtis
        intent_dict: "{base.intent_dict}"
        slot_dict: "{base.slot_dict}"
      gcn_output_dim: 256
      gcn_dropout_rate: 0.5

    intent_classifier:
      _model_target_: model.decoder.classifier.MLPClassifier
      mode: "token-level-intent"
      mlp:
        - _model_target_: torch.nn.Linear
          in_features: "{model.decoder.interaction.output_dim}"
          out_features: 256
        - _model_target_: torch.nn.LeakyReLU
          negative_slope: 0.2
        - _model_target_: torch.nn.Linear
          in_features: 256
          out_features: "{base.intent_label_num}"
      loss_fn:
        _model_target_: torch.nn.BCEWithLogitsLoss
      dropout_rate: 0.4
      use_multi: "{base.multi_intent}"
      multi_threshold: 0.5
      return_sentence_level: true
      ignore_index: "{base.ignore_index}"
      weight: 0.2

    slot_classifier:
      _model_target_: model.decoder.classifier.MLPClassifier
      mode: "slot"
      mlp:
        - _model_target_: torch.nn.Linear
          in_features: "{model.decoder.interaction.output_dim}"
          out_features: "{model.decoder.interaction.output_dim}"
        - _model_target_: torch.nn.LeakyReLU
          negative_slope: 0.2
        - _model_target_: torch.nn.Linear
          in_features: "{model.decoder.interaction.output_dim}"
          out_features: "{base.slot_label_num}"
      ignore_index: "{base.ignore_index}"
      dropout_rate: 0.4
      use_multi: false
      multi_threshold: 0.5
      weight: 0.8
      return_sentence_level: false